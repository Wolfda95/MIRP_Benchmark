# Inference

### Prerequisite: 
- Downloaded MIRP Benchmark Dataset (Instructions: [`1_dataset_guide/`](https://github.com/Wolfda95/MIRP_Benchmark/tree/main/1_dataset_guide))
- For open models: Nvidia GPU with sufficient memory to run the model (We used A6000 48GB) || For proprietary models: API key

<br/>

### Run Inference: 
Each model is executed via a separate script.  <br>
Each script passes one image-question pair after another throught the model. The model answers are saved in a json file. <br>
You find detailed instructions on how to run the script for each model below in the toggel bars. 

<br/>

### Next Step: 
After inference is completed and the model answers are saved in the json file, proceed to [`3_evaluation_code/`](https://github.com/Wolfda95/MIRP_Benchmark/tree/main/3_evaluation_code) to evaluate how many answers are correct and to calculate the statistics. 

<br/> <br/>
# How to run the Open Models


# How to run the Proprietary Models




